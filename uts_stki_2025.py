# -*- coding: utf-8 -*-
"""UTS_STKI_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFtSxFZ63ZHHjVR4lztLqJUTEUbdLkVs
"""

from google.colab import drive
import os

# Mount Google Drive
print("Menghubungkan Google Drive...")
drive.mount('/content/drive')
print("✅ Drive berhasil terhubung!")

# --- SESUAIKAN JALUR INI DENGAN LOKASI FOLDER STKI ANDA ---
# Contoh Asumsi: Folder STKI berada langsung di 'My Drive'
drive_folder_path = '/content/drive/MyDrive/STKI/'
# -------------------------------------------------------------

# List nama file yang akan dibaca
doc_files = [f'D{i}.txt' for i in range(1, 6)]
# Dictionary untuk menyimpan konten dokumen mentah
corpus = {}

# Variabel untuk menampung total karakter
total_chars = 0

print("\n--- Membaca Korpus dari Google Drive ---")

if not os.path.exists(drive_folder_path):
    print(f"❌ ERROR: Jalur folder '{drive_folder_path}' tidak ditemukan. Mohon cek kembali nama folder dan path Anda.")
else:
    for file_name in doc_files:
        full_path = os.path.join(drive_folder_path, file_name)

        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()

            corpus[file_name] = content

            # --- TAMBAHAN: Menghitung Total Karakter ---
            char_count = len(content)
            total_chars += char_count
            # ------------------------------------------

            # Verifikasi singkat
            snippet = content[:50].replace('\n', ' ').strip()
            print(f"✅ {file_name} berhasil dibaca. {char_count} karakter. Isi awal: '{snippet}...'")

        except FileNotFoundError:
            print(f"❌ {file_name} tidak ditemukan di: {full_path}")
        except Exception as e:
            print(f"❌ Gagal membaca {file_name}: {e}")

# Verifikasi akhir
print(f"\nKorpus Berhasil Dimuat! Jumlah dokumen yang berhasil dibaca: {len(corpus)}")

# --- TAMBAHAN: Menampilkan Total Karakter Keseluruhan ---
print(f"✅ Total Karakter Keseluruhan (D1-D5): {total_chars} karakter.")
# --------------------------------------------------------

# Instalasi pustaka yang diperlukan
!pip install Sastrawi nltk scikit-learn
import nltk

# --- PERBAIKAN: Unduh resource 'stopwords' yang hilang ---
nltk.download('punkt')
nltk.download('stopwords') # BARIS INI DITAMBAHKAN
nltk.download('punkt_tab') # Menambahkan baris ini untuk mengunduh punkt_tab
# --------------------------------------------------------

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.corpus import stopwords
# ... impor lainnya

import re
import os
import nltk
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import pandas as pd

# Inisialisasi Tools
stemmer = StemmerFactory().create_stemmer()
# Ambil daftar stopword Bahasa Indonesia dari NLTK
STOPWORDS_ID = set(stopwords.words('indonesian'))

# Tambahkan stopwords kustom yang sering muncul di konteks kampus/berita
CUSTOM_STOPWORDS = {"cite", "source", "fullcontent", "penulis", "editor", "foto", "humas", "dok", "pribadi", "yang", "dan", "di", "ini", "itu", "dengan", "adalah", "untuk", "dari", "melalui", "ke", "pada", "sebagai"}
STOPWORDS_ID.update(CUSTOM_STOPWORDS)

import re
# (Pastikan Anda mengimpor Sastrawi, nltk, dll. di awal notebook)
# from nltk.tokenize import word_tokenize
# ...

def clean(text):
    """Case-folding, normalisasi, dan penghapusan angka/simbol."""

    # Menghapus tag source/cite (Perbaikan Syntax dan Pattern yang dimaksud)
    # Gunakan ekspresi reguler yang benar untuk menghapus tag atau
    # Atau, jika Anda hanya ingin menghapus karakter | (pipe), gunakan: text = re.sub(r'\|', ' ', text)
    # Saya akan menggunakan pembersihan tag source yang sudah kita sepakati sebelumnya untuk hasil yang lebih baik:
    text = re.sub(r'\|', ' ', text)

    # Case Folding
    text = text.lower()

    # Menghapus tanda baca, angka, dan simbol (pertahankan a-z dan spasi)
    text = re.sub(r'[^a-z\s]', ' ', text)

    # Menghapus spasi berlebih
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def tokenize(text):
    """Tokenisasi menggunakan NLTK."""
    # word_tokenize perlu diimpor dari nltk.tokenize
    # Jika tidak ada error, Anda bisa meninggalkannya seperti ini
    return word_tokenize(text)

def remove_stopwords(tokens):
    """Stopword removal dan filter token < 3 huruf."""
    return [token for token in tokens if token not in STOPWORDS_ID and len(token) > 2]

def stem(tokens):
    """Stemming menggunakan Sastrawi."""
    return [stemmer.stem(token) for token in tokens]

def full_preprocess(text):
    """Menjalankan seluruh tahapan preprocessing."""
    text_clean = clean(text)
    tokens = tokenize(text_clean)
    tokens_no_stop = remove_stopwords(tokens)
    tokens_stemmed = stem(tokens_no_stop)
    return tokens_stemmed

# --- Eksekusi dan Penyimpanan ---

doc_files = [f'D{i}.txt' for i in range(1, 6)]
processed_corpus = {} # Dictionary untuk menyimpan token yang sudah diproses

# Buat folder data/processed
output_dir = 'data/processed'
os.makedirs(output_dir, exist_ok=True)

print("--- Memulai Preprocessing ---")
for doc_id in doc_files:
    if doc_id in corpus:
        # Lakukan preprocessing
        raw_content = corpus[doc_id]
        processed_tokens = full_preprocess(raw_content)
        processed_corpus[doc_id] = processed_tokens

        # Simpan keluaran akhir ke data/processed/*.txt
        with open(os.path.join(output_dir, doc_id), 'w', encoding='utf-8') as f_out:
            f_out.write(' '.join(processed_tokens))

        print(f"✅ {doc_id} diproses dan disimpan ke {output_dir}/{doc_id}. Token count: {len(processed_tokens)}")
    else:
        print(f"❌ {doc_id} dilewati karena tidak ada dalam dictionary 'corpus'.")

# --- Uji: Before/After pada 2 Sampel Dokumen ---

doc_sample_1 = 'D1.txt'
doc_sample_2 = 'D4.txt'

print(f"\n--- UJI SAMPEL 1 ({doc_sample_1}) ---")
if doc_sample_1 in corpus:
    print("BEFORE (Mentah):\n", corpus[doc_sample_1][:300].replace('\n', ' '), "...")
    print("\nAFTER (Processed Tokens, 30 awal):\n", processed_corpus[doc_sample_1][:30], "...")

print(f"\n--- UJI SAMPEL 2 ({doc_sample_2}) ---")
if doc_sample_2 in corpus:
    print("BEFORE (Mentah):\n", corpus[doc_sample_2][:300].replace('\n', ' '), "...")
    print("\nAFTER (Processed Tokens, 30 awal):\n", processed_corpus[doc_sample_2][:30], "...")

# --- Uji: 10 token paling sering per dokumen ---

print("\n--- UJI: 10 TOKEN PALING SERING PER DOKUMEN ---")
for doc_id, tokens in processed_corpus.items():
    token_counts = Counter(tokens)
    print(f"\n{doc_id} (Total Token: {len(tokens)}):")
    for token, count in token_counts.most_common(10):
        print(f"  {token:<15}: {count}")

import pandas as pd
import numpy as np
# Asumsi 'processed_corpus' sudah terisi dan 'full_preprocess' sudah didefinisikan.

# Dapatkan daftar dokumen (D1.txt, D2.txt, ...)
# Pastikan ini diambil dari keys() dari processed_corpus
doc_ids_list = sorted(processed_corpus.keys())

# 1. Bangun Vocabulary
vocabulary = set()
for tokens in processed_corpus.values():
    vocabulary.update(tokens)

# 2. Bangun Inverted Index
inverted_index = {}
for term in vocabulary:
    postings = []
    for doc_id, tokens in processed_corpus.items():
        if term in tokens:
            postings.append(doc_id)
    inverted_index[term] = postings

print(f"--- Inverted Index Sample ---")
print(f"Total Vocabulary: {len(vocabulary)} terms")
print(f"udinus postings: {inverted_index.get('udinus', [])}")
print(f"prestasi postings: {inverted_index.get('prestasi', [])}")

# Fungsi Set Operations
def boolean_and(p1, p2):
    """Interseksi (AND)"""
    return sorted(list(set(p1) & set(p2)))

def boolean_or(p1, p2):
    """Union (OR)"""
    return sorted(list(set(p1) | set(p2)))

def boolean_not(p):
    """Komplemen (NOT)"""
    all_docs = set(doc_ids_list)
    return sorted(list(all_docs - set(p)))

# Fungsi Parser dan Pencarian Boolean Sederhana
def search_boolean_simple(query):
    """Mengeksekusi query Boolean sederhana (TERM1 OPERATOR TERM2)."""
    query_parts = query.lower().split()

    # Preprocess terms menggunakan fungsi dari SOAL 02
    term1_processed = full_preprocess(query_parts[0]) if len(query_parts) > 0 else []
    term1 = term1_processed[0] if term1_processed else ''

    operator = query_parts[1].upper() if len(query_parts) > 1 else 'AND'

    term2_processed = full_preprocess(query_parts[2]) if len(query_parts) > 2 else []
    term2 = term2_processed[0] if term2_processed else ''

    # Ambil postings list
    p1 = inverted_index.get(term1, [])
    p2 = inverted_index.get(term2, [])

    explanation = f"Pencarian: {term1} {operator} {term2}"
    result = []

    # Validasi term
    if term1 not in vocabulary:
        return [], f"Term 1: '{term1}' tidak ditemukan dalam vocabulary."
    if operator != 'NOT' and term2 not in vocabulary:
        return [], f"Term 2: '{term2}' tidak ditemukan dalam vocabulary."

    # Eksekusi Operator
    if operator == 'AND':
        result = boolean_and(p1, p2)
        explanation += f" ({term1} postings: {p1}) INTERSEKSI ({term2} postings: {p2})"
    elif operator == 'OR':
        result = boolean_or(p1, p2)
        explanation += f" ({term1} postings: {p1}) UNION ({term2} postings: {p2})"
    elif operator == 'NOT':
        # Implementasi TERM1 NOT TERM2
        result = sorted(list(set(p1) - set(p2)))
        explanation += f" ({term1} postings: {p1}) MINUS ({term2} postings: {p2})"
    else:
        explanation = f"Operator {operator} tidak didukung. Gunakan AND/OR/NOT."

    return result, explanation

# --- Uji dengan 3 Query ---
print("\n--- Uji Query Boolean ---")

q_test_1 = "mahasiswa AND prestasi"
r1, exp1 = search_boolean_simple(q_test_1)
print(f"\nQuery 1: {q_test_1}")
print(f"Hasil: {r1}")
print(f"Explain: {exp1}")

q_test_2 = "ormawa OR visitasi"
r2, exp2 = search_boolean_simple(q_test_2)
print(f"\nQuery 2: {q_test_2}")
print(f"Hasil: {r2}")
print(f"Explain: {exp2}")

q_test_3 = "dosen NOT ilkom"
r3, exp3 = search_boolean_simple(q_test_3)
print(f"\nQuery 3: {q_test_3}")
print(f"Hasil: {r3}")
print(f"Explain: {exp3}")

# --- Uji Wajib: Precision/Recall Boolean ---
print("\n--- Evaluasi Boolean Retrieval (Precision/Recall/F1) ---")

# Definisikan Gold Relevant Docs (Truth Set)
gold_sets = {
    # D1, D2, D4 relevan dengan prestasi mahasiswa
    "prestasi AND mahasiswa": [f'D{i}.txt' for i in [1, 2, 4]],
    # D3 relevan dengan dosen ilkom
    "dosen AND ilkom": ['D3.txt'],
    # D5 relevan dengan stunting sragen
    "stunting AND sragen": ['D5.txt']
}

def calculate_precision_recall(retrieved, relevant):
    """Menghitung Precision, Recall, dan F1-Score."""
    retrieved_set = set(retrieved)
    relevant_set = set(relevant)

    # |Relevant ∩ Retrieved| (True Positives)
    intersection = len(retrieved_set.intersection(relevant_set))

    # Precision = TP / |Retrieved|
    precision = intersection / len(retrieved_set) if len(retrieved_set) > 0 else 0

    # Recall = TP / |Relevant|
    recall = intersection / len(relevant_set) if len(relevant_set) > 0 else 0

    # F1 Score
    F1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, F1

for query, relevant_docs in gold_sets.items():
    retrieved_docs, _ = search_boolean_simple(query)
    P, R, F1 = calculate_precision_recall(retrieved_docs, relevant_docs)

    print(f"\nQuery: {query}")
    print(f"  Gold Relevant Docs: {relevant_docs}")
    print(f"  Boolean Result Set: {retrieved_docs}")
    print(f"  Precision: {P:.4f}, Recall: {R:.4f}, F1-Score: {F1:.4f}")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
# Asumsi 'processed_corpus', 'corpus', 'full_preprocess', dan 'doc_ids_list' sudah tersedia.

# --- Langkah 1: Hitung TF-IDF Matriks ---

# Ubah token yang sudah diproses kembali menjadi string dokumen untuk Vectorizer
docs_string = [' '.join(tokens) for tokens in processed_corpus.values()]
doc_ids_list_vsm = list(processed_corpus.keys())

# Inisialisasi TfidfVectorizer
vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)

# Bentuk TF-IDF Matriks Dokumen (Sparse Matrix)
tfidf_matrix = vectorizer.fit_transform(docs_string)

print("\n--- SOAL 04: Vector Space Model & Ranking ---")
print(f"Bentuk Matriks (Dokumen x Term): {tfidf_matrix.shape}")
print(f"Total Term/Fitur: {len(vectorizer.get_feature_names_out())}")

# --- Langkah 2 & 3: Representasi Query, Cosine Similarity, dan Ranking ---

def search_vsm(query, k=3):
    # Preprocess query dan ubah menjadi string
    processed_query = ' '.join(full_preprocess(query))
    if not processed_query:
        return pd.DataFrame(), "Query kosong setelah preprocessing."

    # Representasi query sebagai vektor TF-IDF
    query_vector = vectorizer.transform([processed_query])

    # Hitung cosine similarity
    cosine_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()

    # Buat DataFrame hasil
    results = pd.DataFrame({
        'doc_id': doc_ids_list_vsm,
        'cosine_similarity': cosine_scores
    })

    # Ranking: Urutkan berdasarkan skor tertinggi dan ambil top-k
    results = results.sort_values(by='cosine_similarity', ascending=False)
    top_k_results = results.head(k)

    # Tambahkan snippet 120 char
    top_k_results['snippet'] = top_k_results['doc_id'].apply(
        lambda x: corpus[x][:120].replace('\n', ' ').strip() + '...'
    )

    return top_k_results, "Pencarian VSM berdasarkan Cosine Similarity (TF-IDF)."

# --- Uji dengan Query VSM ---
q_vsm_1 = "inovasi aplikasi stunting"
top_k_df, exp = search_vsm(q_vsm_1, k=3)

print(f"\n--- Uji VSM TOP-3 Hasil untuk Query: '{q_vsm_1}' ---")
print(exp)
print(top_k_df[['doc_id', 'cosine_similarity', 'snippet']])

# --- Langkah 4: Uji Wajib VSM (Precision@k, MAP@k) ---

print("\n--- Evaluasi VSM (Precision@k dan MAP@k) ---")

# Gold Set yang sama dari SOAL 03
vsm_gold_sets = {
    "prestasi mahasiswa udinus": [f'D{i}.txt' for i in [1, 2, 4]],
    "dosen ilkom": ['D3.txt'],
    "stunting sragen": ['D5.txt']
}

k_vsm = 3 # Ambil Top-3

def calculate_ap_at_k(results_df, relevant_docs, k):
    """Menghitung Average Precision (AP) pada k."""

    # Tentukan relevansi biner (1=relevan, 0=tidak)
    results_df['relevance'] = results_df['doc_id'].apply(lambda x: 1 if x in relevant_docs else 0)

    # Ambil hasil top-k
    top_k = results_df.head(k)

    sum_precision = 0
    relevant_count = 0

    for i in range(len(top_k)):
        if top_k.iloc[i]['relevance'] == 1:
            relevant_count += 1
            # Precision@i+1
            precision_at_i = relevant_count / (i + 1)
            sum_precision += precision_at_i

    # Total dokumen relevan dalam GOLD SET
    num_relevant = len(relevant_docs)

    # Average Precision
    ap = sum_precision / num_relevant if num_relevant > 0 else 0

    # Precision@k (hanya pada posisi k)
    pk = relevant_count / k if k > 0 else 0

    return ap, pk

ap_scores = []
pk_scores = []

# Loop melalui setiap gold set
for query, relevant_sets in vsm_gold_sets.items():
    # Ambil semua hasil ranking (k max) untuk perhitungan AP/MAP
    results_df, _ = search_vsm(query, k=len(doc_ids_list_vsm))

    ap, pk = calculate_ap_at_k(results_df, relevant_sets, k=k_vsm)

    ap_scores.append(ap)
    pk_scores.append(pk)

    print(f"\nQuery: '{query}' (k={k_vsm})")
    print(f"  P@{k_vsm}: {pk:.4f}, AP: {ap:.4f}")

# Hitung MAP@k
map_at_k = np.mean(ap_scores)
print(f"\n*** Mean Average Precision (MAP@{k_vsm}): {map_at_k:.4f} ***")
print(f"Rata-rata Precision@{k_vsm}: {np.mean(pk_scores):.4f}")